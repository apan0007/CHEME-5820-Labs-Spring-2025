{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32fe3c6d-09ea-4c0e-81f7-3c8454d05e77",
   "metadata": {},
   "source": [
    "# L5d: Kernelized Support Vector Machines (kSVMs)\n",
    "In this lab, we will experiment with a kernel Support Vector Machine (kSVM) to classify the non-linearly separable datasets we construct. In particular, we'll look at a kernelized version of the soft-margin support vector machine. If these terms are unfamiliar, [check out the L5c notes](https://github.com/varnerlab/CHEME-5820-Lectures-Spring-2025/blob/main/lectures/week-5/L5c/docs/Notes.pdf) and the review below.\n",
    "\n",
    "### Theory: Support Vector Machine (SVM)\n",
    "Suppose, we have dataset $\\mathcal{D} = \\{(\\hat{\\mathbf{x}}_{i}, y_{i}) \\mid i = 1,2,\\dots,n\\}$, where $\\hat{\\mathbf{x}}_i \\in \\mathbb{R}^p$ is an _augmented_ feature vector ($m$ features with additional `1` to model the bias on the end of the vector) and $y_i \\in \\{-1, 1\\}$ is the corresponding class label. The goal of an SVM (for binary classification tasks) is to find the hyperplane $\\mathcal{H}(\\hat{\\mathbf{x}}) = \\{\\hat{\\mathbf{x}} \\mid \\left<\\hat{\\mathbf{x}},\\theta\\right> = 0\\}$ that separates the data points into two classes (those points above the hyperplane, and those points below the hyperplane), where $\\theta \\in \\mathbb{R}^{p}$ ($p=m+1$) is the normal vector to the hyperplane, or the parameters of the model that we need to estimate.\n",
    "* __Why another method__? Support vector machines (SVMs) and other approaches, e.g., [the perceptron](https://en.wikipedia.org/wiki/Perceptron) differ primarily in their optimization objectives and training methods: while a [perceptron](https://en.wikipedia.org/wiki/Perceptron) can find _a hyperplane_ that separates classes, SVMs seek to find the _best hyperplane_ in the sense that the _margin_ between classes is maximized.\n",
    "\n",
    "#### Soft margin\n",
    "Let's take a look at a [schematic of the ideas behind a soft margin support vector machine](https://github.com/varnerlab/CHEME-5820-Lectures-Spring-2025/blob/main/lectures/week-5/L5c/docs/figs/Fig-SVM-Schematic-Softmargin.pdf).\n",
    "If the data is _not linearly separable_, then we know that a perfect $\\mathcal{H}(\\hat{\\mathbf{x}})$ will not exist, i.e., \n",
    "no hyperplane will separate the data without making at least one mistake. In this case, we can estimate the _best_ hyperplane possible by solving the maximum soft margin problem given by:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\min_{\\theta}\\quad & \\frac{1}{2}\\lVert{\\theta}\\rVert_{2}^{2} + C\\sum_{i=1}^{n}\\xi_{i}\\\\\n",
    "    \\text{subject to}\\quad & y_{i}\\left<\\hat{\\mathbf{x}}_{i},\\theta\\right> \\geq 1 - \\xi_{i}\\quad\\forall i\\\\\n",
    "    & \\xi_{i} \\geq 0\\quad\\forall i\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\xi_{i}$ is a _slack variable_, that quantifies the cost of a classification mistake, and $C>{0}$ is a user-adjustable parameter that controls the trade-off between maximizing the margin and minimizing the slack variables.\n",
    "* __Values of $C$__: If $C\\gg{1}$ the classifier will behave like the maximum (hard) margin classifier, i.e., mistakes will be expensive, and the search will avoid making choices with mistakes. However, if $C\\ll{1}$, the classifier will allow more slack (mistakes), i.e., mistakes are cheap, so what's it matter!\n",
    "\n",
    "### Tasks\n",
    "Fill me in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ef21be-0201-4515-89bb-ba8112541d8e",
   "metadata": {},
   "source": [
    "## Task 1: Setup, Data, and Prerequisites\n",
    "We set up the computational environment by including the `Include.jl` file, loading any needed resources, such as sample datasets, and setting up any required constants. \n",
    "* The `Include.jl` file also loads external packages, various functions that we will use in the exercise, and custom types to model the components of our problem. It checks for a `Manifest.toml` file; if it finds one, packages are loaded. Other packages are downloaded and then loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "070be284-e10c-4c83-90bb-4cab39d97ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Include.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be56e42-f2dd-4da0-a8df-b1bae63621eb",
   "metadata": {},
   "source": [
    "### Data\n",
    "In this section, we will build sample (binary) datasets $\\mathcal{D} = \\{(\\hat{\\mathbf{x}}_{i}, y_{i}) \\mid i = 1,2,\\dots,n\\}$ to classify. Before we get started, we specify some constants that we use later:\n",
    "* The `number_of_points_per_label::Int` variable controls the number of sample points in the dataset for each label. The total number of instances in $\\mathcal{D}$ will be two times this number.\n",
    "* The `number_of_training_examples::Int` variable controls the number of points we use for `training`, with the balance of the instances then going to `test`.\n",
    "* The `number_of_features::Int` variable controls the number of features in the data, i.e., for $\\mathbf{x}\\in\\mathbb{R}^{m}$, this value is `m`.\n",
    "* The `ϵ::Float64` variable is a threshold value for including a not linearly separable data point into the dataset $\\mathcal{D}$, i.e., it is the probability that if we generate data that is not separable, we include it in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb75e970-d8fd-467e-8793-4122fce566ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_points_per_label = 1000; # number of samples per label (2 x this is the total)\n",
    "number_of_training_examples = 1200; # pick this many training examples at random\n",
    "number_of_features = 6; # we have this many features\n",
    "ϵ = 0.80; # threshold of random NLS points to accept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542a6e99-99b0-44c6-9266-8c473960f782",
   "metadata": {},
   "source": [
    "Let's set up the color dictionary to visualize the classification datasets. The keys of the `my_color_dictionary::Dict Int64, RGB` dictionary class labels, i.e., $ y\\in\\{-1,1\\}$ while the values are the colors mapped to that label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8bf8d35-4f28-448e-aa49-4fa6ae812b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_color_dictionary = Dict{Int64,RGB}();\n",
    "my_color_dictionary[1] = colorant\"#03045e\"; # color for Label = 1\n",
    "my_color_dictionary[-1] = colorant\"#e36414\"; # color for Label = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668d9e68-3fde-4b32-bf85-b34be4f508d3",
   "metadata": {},
   "source": [
    "Next, let's write a code to generate a [non-lineraly separable data set](https://en.wikipedia.org/wiki/Linear_separability) $\\mathcal{D}$ that we'll use to train (and test) our [kSVM](https://en.wikipedia.org/wiki/Support_vector_machine#Nonlinear_kernels).\n",
    "\n",
    "* First, we generate a random parameter vector $\\theta\\in\\mathbb{R}^{p}$ where $p = m+1$.\n",
    "* Next, we generate (random) augmented feature vecotors $\\hat{\\mathbf{x}}\\in\\mathbb{R}^{p}$ and for specified labels, and check the $y\\cdot\\left(\\hat{\\mathbf{x}}^{\\top}\\mathbf{\\theta}\\right) \\geq 0$ condition. If this condition is true, we `accept` that data; otherwise, we `reject` the data. If we `accept` the data, we store it in the `dataset` variable, a [Set](https://docs.julialang.org/en/v1/base/collections/#Base.Set) holding a [NamedTuple type](https://docs.julialang.org/en/v1/base/base/#Core.NamedTuple), which is a mix of a tuple and a dictionary. If we `reject` the data, we roll a random number and compare it to the `ϵ::Float64` threshold value. If the random number is less than or equal to `ϵ::Float64`, we keep the rejected sample (and add it to the sample archive).\n",
    "* We keep iterating the loop until the number of elements of the `dataset` is greater than or equal to the desired number of test points specified in the `number_of_points_per_label` variable.\n",
    "\n",
    "We return the dataset $\\mathcal{D}$ in the `D::Array{Float64,2}` array, where each row is an example, each column is a feature, except for the last column, which is the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4099b294-ce46-414c-98b4-69ea85e3743a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000×7 Matrix{Float64}:\n",
       "  0.354229    1.48583      0.357261  -0.138804   -1.16014     0.155363   -1.0\n",
       "  1.01153     0.465517    -0.157728   0.485708    0.829448    0.688443   -1.0\n",
       "  0.0997475   0.73565      0.603192  -0.206756    0.204424   -0.587373    1.0\n",
       "  0.24088    -0.989572     0.562111  -0.11954    -0.307492   -0.0831538   1.0\n",
       " -1.05312    -0.118803    -0.420808   1.24608     0.43835    -0.0191762  -1.0\n",
       " -0.596113    0.919406    -0.324427   0.635136   -1.17562    -0.766255    1.0\n",
       "  1.70651    -2.21561      0.255294  -0.437363   -1.4705     -0.190167   -1.0\n",
       "  0.125424    0.450443     0.661127  -1.52093     0.401168    2.13848     1.0\n",
       " -1.4095     -0.644374    -0.437355   0.118989   -0.71492     0.776252   -1.0\n",
       " -0.934505    2.37437     -0.693144   0.180891    1.18009    -0.20485    -1.0\n",
       " -0.213116    0.649262    -0.463989  -1.51625    -1.85898     0.597768   -1.0\n",
       " -0.606698   -0.11543      1.70487    0.139098   -0.680176   -0.642914    1.0\n",
       "  0.0903684  -1.26194      1.25665   -0.892676   -0.0698749  -0.184953    1.0\n",
       "  ⋮                                                           ⋮          \n",
       " -0.886869   -1.36017     -1.79524    0.831241    1.1797     -0.29915     1.0\n",
       "  1.15106     0.494663     0.427102  -0.578723    1.14959     1.28386    -1.0\n",
       "  0.335868   -0.373797    -1.31597    0.202671    0.588546   -1.61729    -1.0\n",
       " -0.10367     3.29371     -0.169683  -0.526005   -0.347978   -0.185423   -1.0\n",
       "  0.873128   -1.43477     -1.26925    1.84091     0.8205      1.10911    -1.0\n",
       " -0.786209    0.0995003    0.575551  -1.69196     1.36475     1.6778      1.0\n",
       "  1.28532     1.29638      1.11556    0.0423756  -0.518156    0.595704    1.0\n",
       "  0.960105    0.00946244  -0.762369   1.40898    -0.282872   -1.17967     1.0\n",
       " -2.17882    -0.484183     1.21211   -0.0327305  -0.773589    0.448055    1.0\n",
       " -0.755576    2.16858      0.255085   0.371191   -0.778672   -0.353272   -1.0\n",
       "  0.502395    0.877597    -0.629623  -0.572871    0.710471    2.58631    -1.0\n",
       " -0.229176   -1.15242      0.625582  -0.499778    0.655316   -0.722619    1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = let\n",
    "\n",
    "    # initialize \n",
    "    dataset = Set{@NamedTuple{x::Array{Float64,1},y::Int64}}();\n",
    "    w = randn(number_of_features+1); # tmp parameters that we use to make data\n",
    "\n",
    "    # Logic to generate y = 1 samples\n",
    "    should_keep_looping = true;\n",
    "    while (should_keep_looping == true)\n",
    "    \n",
    "        x = randn(number_of_features) |> x -> push!(x,1); # generate a random augmented feature vector\n",
    "        y = 1; # specify the label\n",
    "        \n",
    "        # check -\n",
    "        if (y*(sum(w.*x)) ≥ 0.0)\n",
    "            data = (x = x, y = y); # use a tuple, and set to enforce unique\n",
    "            push!(dataset,data);\n",
    "        else\n",
    "            # fails, should we keep this data?\n",
    "            if (rand() ≤ ϵ)\n",
    "                data = (x = x, y = y); # use a tuple, and set to enforce unique\n",
    "                push!(dataset,data);\n",
    "            end\n",
    "        end\n",
    "    \n",
    "        # if we have enough examples, stop iterating\n",
    "        if (length(dataset) >= number_of_points_per_label)\n",
    "            should_keep_looping = false\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # Logic to generate y = -1 samples\n",
    "    should_keep_looping = true;\n",
    "    while (should_keep_looping == true)\n",
    "    \n",
    "        x = randn(number_of_features) |> x -> push!(x,1); # generate a random augmented feature vector\n",
    "        y = -1;\n",
    "        \n",
    "        # check -\n",
    "        if (y*(sum(w.*x)) ≥ 0.0) # this is a LS point, keep\n",
    "            data = (x = x, y = y); # use a tuple, and set to enforce unique\n",
    "            push!(dataset,data);\n",
    "        else\n",
    "            # fails, should we keep this data?\n",
    "            if (rand() ≤ ϵ)\n",
    "                data = (x = x, y = y); # use a tuple, and set to enforce unique\n",
    "                push!(dataset,data);\n",
    "            end\n",
    "        end\n",
    "    \n",
    "        # If we have enough examples, stop iterating\n",
    "        if (length(dataset) >= 2*number_of_points_per_label)\n",
    "            should_keep_looping = false\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # ok, so let's put this data into a matrix -\n",
    "    D = Array{Float64,2}(undef, 2*number_of_points_per_label, number_of_features+1);\n",
    "    for i ∈ 1:2*number_of_points_per_label\n",
    "        example = pop!(dataset);\n",
    "        feature = example.x;\n",
    "        label = example.y;\n",
    "        for j ∈ 1:number_of_features\n",
    "            D[i,j] = feature[j];\n",
    "        end\n",
    "        D[i,end] = label;\n",
    "    end\n",
    "    D\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263a91da-873b-4d35-b339-4848d555c236",
   "metadata": {},
   "source": [
    "### Visualize dataset `D`\n",
    "`Unhide` the code block below to see how we plotted the dataset `D`, which contains two continuous features and a label. The color indicates the label.\n",
    "* __Summary__: We will get a different pattern of $\\pm{1}$ labels depending the `ϵ::Float64` value. The dark blue dots represent label `1`, while the orange data represents label `1`. Our classifier should be able to learn the mapping between the features and the labels for linearly separable datasets but may struggle with random outlier points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7302bcbf-7bce-4d74-9619-5889a0ca8af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mVisualization disabled for more than 2 features!\n"
     ]
    }
   ],
   "source": [
    "let\n",
    "\n",
    "    dataset = D; # what dataset am I looking at?\n",
    "    number_of_points_to_plot = size(dataset,1);\n",
    "    if (number_of_features > 2)\n",
    "        @info \"Visualization disabled for more than 2 features!\"\n",
    "    else\n",
    "        p = plot(bg=\"gray95\", background_color_outside=\"white\", framestyle = :box, fg_legend = :transparent); # make an empty plot\n",
    "    \n",
    "        # plot label = 1\n",
    "        testlabel = 1;\n",
    "        i = findfirst(label -> label == testlabel,  dataset[:,3])\n",
    "        c = my_color_dictionary[testlabel]\n",
    "        scatter!([dataset[i,1]], [dataset[i,2]], label=\"Label: $(testlabel)\", c=c)\n",
    "    \n",
    "        # plot label = -1\n",
    "        testlabel = -1;\n",
    "        i = findfirst(label -> label == testlabel,  dataset[:,3])\n",
    "        c = my_color_dictionary[testlabel]\n",
    "        scatter!([dataset[i,1]], [dataset[i,2]], label=\"Label: $(testlabel)\", c=c)\n",
    "    \n",
    "        # plot all points\n",
    "        for i ∈ 1:number_of_points_to_plot\n",
    "            label = dataset[i,3]; # label\n",
    "            c = my_color_dictionary[label]\n",
    "            scatter!([dataset[i, 1]], [dataset[i, 2]], label=\"\", mec=:navy, c=c)\n",
    "        end\n",
    "        \n",
    "        xlabel!(\"Feature 1 (AU)\", fontsize=18);\n",
    "        ylabel!(\"Feature 2 (AU)\", fontsize=18);\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fa2adc-4611-442e-8d59-815186227901",
   "metadata": {},
   "source": [
    "Next, let's split that dataset `D` into `training` and `test` subsets. We do this randomly, where the `number_of_training_examples::Int64` variable specifies the number of training points. The `training::Array{Float64,2}` data will be used to estimate the model parameters, and `test::Array{Float64,2}` will be used for model testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a138b06c-0106-4e55-b68e-bdd561076c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "training, test = let\n",
    "\n",
    "    number_of_features = size(D,2); # number of cols of housing data\n",
    "    number_of_examples = size(D,1); # number of rows of housing data\n",
    "    full_index_set = range(1,stop=number_of_examples,step=1) |> collect |> Set;\n",
    "    \n",
    "    # build index sets for training and testing\n",
    "    training_index_set = Set{Int64}();\n",
    "    should_stop_loop = false;\n",
    "    while (should_stop_loop == false)\n",
    "        i = rand(1:number_of_examples);\n",
    "        push!(training_index_set,i);\n",
    "\n",
    "        if (length(training_index_set) == number_of_training_examples)\n",
    "            should_stop_loop = true;\n",
    "        end\n",
    "    end\n",
    "    test_index_set = setdiff(full_index_set,training_index_set);\n",
    "\n",
    "    # build the test and train datasets -\n",
    "    training = D[training_index_set |> collect,:];\n",
    "    test = D[test_index_set |> collect,:];\n",
    "\n",
    "    # return\n",
    "    training, test\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383ff19e-5964-4fc5-90c8-0f0577ba06bc",
   "metadata": {},
   "source": [
    "## Task 2: Classification using an SVM\n",
    "In this task, we [use the SVM implementation exported by the `LIBSVM.jl` package](https://github.com/JuliaML/LIBSVM.jl) to classify the dataset $\\mathcal{D}$ generated in task 1. In particular, we use the `training` dataset to estimate the unknown model parameters $\\theta$ [using the `svmtrain(...)` method](https://github.com/JuliaML/LIBSVM.jl/blob/master/src/LIBSVM.jl), and the `test` data to evaluate the performance of the classifier on unseen data [using the `svmpredict(...)` method](https://github.com/JuliaML/LIBSVM.jl/blob/master/src/LIBSVM.jl). \n",
    "* The [`svmtrain(...)` method](https://github.com/JuliaML/LIBSVM.jl/blob/master/src/LIBSVM.jl) takes an augmented training examples matrix $\\hat{\\mathbf{X}}^{\\top}$ where the examples are on the columns and the features are the rows, and a label vector $\\mathbf{y}\\in\\left\\{-1,1\\right\\}$.\n",
    "* The [`svmtrain(...)` method](https://github.com/JuliaML/LIBSVM.jl/blob/master/src/LIBSVM.jl) returns a [model instance](https://github.com/JuliaML/LIBSVM.jl/blob/master/src/LIBSVM.jl) that holds the trained data and a bunch of other data associated with the problem.\n",
    "* __Hmmm__: One of the (super) interesting optional arguments [the `svmtrain(...)` method](https://github.com/JuliaML/LIBSVM.jl/blob/master/src/LIBSVM.jl) is the `kernel` argument. Check out the documentation to see what kernels are supported! Wow! we get [kernelized SVM capability](https://en.wikipedia.org/wiki/Support_vector_machine#Nonlinear_kernels) right out of the box. _Buy versus build, 99% buy!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d9c5e427-8dce-47b2-a1d3-51fd7bcfbbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*.*\n",
      "optimization finished, #iter = 1084\n",
      "nu = 0.891559\n",
      "obj = -1002.820119, rho = -0.202852\n",
      "nSV = 1110, nBSV = 1029\n",
      "Total nSV = 1110\n"
     ]
    }
   ],
   "source": [
    "model = let\n",
    "\n",
    "    # Setup the data that we are using\n",
    "    D = training; # what dataset are we looking at?\n",
    "    number_of_examples = size(D,1); # how many rows?\n",
    "    X = [D[:,1:end-1] ones(number_of_examples)] |> transpose |> Matrix; # augmented features (arranged as m x n)\n",
    "    y = D[:,end]; # label\n",
    "\n",
    "    # Train the data -\n",
    "    model = svmtrain(X, y, kernel=LIBSVM.Kernel.RadialBasis, verbose = true); # we are using the LIBSVM\n",
    "\n",
    "    # return\n",
    "    model\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eb0c2d95-4cd2-459b-8a16-3edd1b364375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0a40341-4c8c-4511-8a6f-814182d10735",
   "metadata": {},
   "source": [
    "__Inference__: Now that we have parameters estimated from the `training` data, we can use those parameters on the `test` dataset to see how well the model can differentiate between an actual banknote and a forgery on data it has never seen. We run the classification operation on the (unseen) test data [using the `svmpredict(...)` method](https://github.com/JuliaML/LIBSVM.jl/blob/master/src/LIBSVM.jl). \n",
    "* The [`svmpredict(...)` method](https://github.com/JuliaML/LIBSVM.jl/blob/master/src/LIBSVM.jl) returns the predicted label which we store in the `ŷ::Array{Int64,1}` array. We store the actual (correct) label in the `y::Array{Int64,1}` vector.\n",
    "* The [`svmpredict(...)` method](https://github.com/JuliaML/LIBSVM.jl/blob/master/src/LIBSVM.jl) also returns a second output which we save in the `decision_values` variable. __Hmmmm__. Not sure what these values are ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b42d7969-9bfa-4f4f-98fb-ea9443c11eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ,y,d = let\n",
    "\n",
    "     # Setup the data that we are using\n",
    "    D = test; # what dataset are we looking at?\n",
    "    number_of_examples = size(D,1); # how many rows?\n",
    "    X = [D[:,1:end-1] ones(number_of_examples)] |> transpose |> Matrix; # features (arranged as m x n)\n",
    "    y = D[:,end]; # label\n",
    "    \n",
    "    # Test model on the other half of the data.\n",
    "    ŷ, decision_values = svmpredict(model, X);\n",
    "\n",
    "    # return -\n",
    "    ŷ,y,decision_values\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efea4db2-103c-4e8f-b858-96dd1fb66db5",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "Finally, let's compute the confusion matrix. The confusion matrix is a $2\\times{2}$ matrix that contains four entries: true positive (TP), false positive (FP), true negative (TN), and false negative (FN). [Click me for a confusion matrix schematic!](https://github.com/varnerlab/CHEME-5820-Labs-Spring-2025/blob/main/labs/week-3/L3b/figs/Fig-BinaryConfusionMatrix.pdf). Let's compute these four values [using the `confusion(...)` method](src/Compute.jl) and store them in the `CM::Array{Int64,2}` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d57ea7ec-1fd8-41cf-9d4d-e85768e10d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Matrix{Int64}:\n",
       " 243  136\n",
       " 242  179"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CM = confusion(y, ŷ) # call with the SVM values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52750870-9c80-4e72-8dfa-6761ba710171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction correct: 0.5275 Fraction incorrect 0.47250000000000003\n"
     ]
    }
   ],
   "source": [
    "number_of_test_points = length(y);\n",
    "correct_prediction_perceptron = CM[1,1] + CM[2,2];\n",
    "(correct_prediction_perceptron/number_of_test_points) |> f-> println(\"Fraction correct: $(f) Fraction incorrect $(1-f)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146a9218-6a6a-4ca4-b6c1-e107d8e2d083",
   "metadata": {},
   "source": [
    "### DQs \n",
    "1. What are all the fields inside the `model` instance, and what do they mean? Let's [check out the code, and see what we see!](https://github.com/JuliaML/LIBSVM.jl/blob/master/src/LIBSVM.jl)\n",
    "2. What are the `decision_values` that are returned from the [`svmpredict(...)` method](https://github.com/JuliaML/LIBSVM.jl/blob/master/src/LIBSVM.jl)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0e2021-5686-473e-8f6b-a4508bb14a31",
   "metadata": {},
   "source": [
    "## Task 3: Implement Grid Search for Optimal Hyperparameters\n",
    "In this task, we perform a grid search on the hyperparameters of the classification model, namely the $C$ parameter in the objective function and the length-scale $\\gamma$ parameter. In the search below, we assume we are using the RBF kernel.\n",
    "\n",
    "Grid search for kernel SVM parameters C and $\\gamma$ involves systematically exploring combinations of these hyperparameters to find the optimal configuration for model performance. Here's a description of the process:\n",
    "* __Define parameter ranges__. For the cost parameter $C$, we use $C\\in\\left\\{2^{-5},2^{-3},\\dots,2^{15}\\right\\}$ while for the length scale parameter $\\gamma$, we use $\\gamma\\in\\left\\{2^{-15},2^{-13},\\dots,2^{3}\\right\\}$. We store the exponents of these ranges in the `α::Array{Float64,1}` and `β::Array{Float64,1}` arrays, respectively.\n",
    "* __New model__: For each parameter combination $(C_{i},\\gamma_{j})$ we train a SVM model with a RBF kernel, compute the confusion matrix and then evaluate the prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "916a6ac0-a18f-4b0e-9aa5-dda1e0fa7dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, α, β = let\n",
    "\n",
    "    # Training data setup -\n",
    "    D₁ = training; # what dataset are we looking at?\n",
    "    number_of_training_examples = size(D₁,1); # how many rows?\n",
    "    X₁ = [D₁[:,1:end-1] ones(number_of_training_examples)] |> transpose |> Matrix; # augmented features (arranged as m x n)\n",
    "    y₁ = D₁[:,end]; # label\n",
    "\n",
    "    # Test data setup -\n",
    "    D₂ = test; # what dataset are we looking at?\n",
    "    number_of_test_examples = size(D₂,1); # how many rows?\n",
    "    X₂ = [D₂[:,1:end-1] ones(number_of_test_examples)] |> transpose |> Matrix; # features (arranged as m x n)\n",
    "    y₂ = D₂[:,end]; # label\n",
    "    \n",
    "    α = range(-5,stop = 15, step=1) |> collect; # exponent for C -\n",
    "    β = range(-15,stop = 3, step=1) |> collect; # exponent for γ -\n",
    "    number_of_points_C = length(α);\n",
    "    number_of_points_gamma = length(β);\n",
    "    accuracy = Array{Float64,2}(undef, number_of_points_C, number_of_points_gamma);\n",
    "    \n",
    "    for i ∈ eachindex(α)\n",
    "        C = 2.0^α[i];\n",
    "        for j ∈ eachindex(β)\n",
    "            γ = 2.0^β[j];\n",
    "\n",
    "            # train model -\n",
    "            ŷ₂,_ = svmtrain(X₁, y₁, kernel=LIBSVM.Kernel.RadialBasis, \n",
    "                verbose = false, cost = C, gamma = γ) |> model -> svmpredict(model,X₂);\n",
    "\n",
    "            # how many mistakes?\n",
    "            CM = confusion(y₂, ŷ₂);\n",
    "            correct_prediction_perceptron = CM[1,1] + CM[2,2];\n",
    "            accuracy[i,j] = (correct_prediction_perceptron/number_of_test_examples);\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    error, α, β\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bfccd256-16f9-4a9f-a649-f25f161fdb53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHIAAAB+CAAAAADddBraAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAAjBJREFUaAW9wVFuG0cARME3y6bkk+fMWoo7Ezz9rEErpGMDXZV/aAt1oS7UhbpQF/7a4LR4LdSFulAX6kJd+At3dKCFBrqiC1o8CnWhLtSFulAX/sgNTTQ5LR5d0OIU6kJdqAt1oS78Tzd0cNrQDV3RRDt6Rxe0UKgLdaEu1IW68Ns+0B1dOE20oYUWGugTHegNhbpQF+pCXagLv+ED3dCGBrqjC6cNXdAdHWigOwp1oS7UhbpQF576QDsaaEMTbZwW2tDitHgU6kJdqAt1oS78hx3tnC5ocFpoog0NtKPB90JdqAt1oS7UhV/saEeDRwstFPSJghYaaKHBo1AX6kJdqAt14Sc72jktNNAFTbTQRANd0URBn3wv1IW6UBfqQl34sqMb37uiiRYaaKEDTXRDF54JdaEu1IW6UJcd7ZwGWmiggRaa6B0daEMDHWjwTKgLdaEu1IW67JwGWmihNzTRQhu6oYWCDjTR4JlQF+pCXagLdeHLQIvTQANNNNGFR1c00Q/0iQbfC3WhLtSFulCXwaOF3tBEg9PG6YYWOtDGa6Eu1IW6UBfqwpfFo4EmWmhDd7ShgTb0iX7wWqgLdaEu1IW68JOJ3tFEG7qjgQY6UNBEC01eC3WhLtSFulAXfjHQQpPThgaaaKCFBrrzWqgLdaEu1IW68GWid3SggRYaaKKghTa00IYmGjwT6kJdqAt1oS4LbWigwWmhgQ40UNCGFgq6o8UzoS7UhbpQF+r+BeXneQS8EjV4AAAAAElFTkSuQmCC",
      "text/html": [
       "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHIAAAB+CAAAAADddBraAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAAjBJREFUaAW9wVFuG0cARME3y6bkk+fMWoo7Ezz9rEErpGMDXZV/aAt1oS7UhbpQF/7a4LR4LdSFulAX6kJd+At3dKCFBrqiC1o8CnWhLtSFulAX/sgNTTQ5LR5d0OIU6kJdqAt1oS78Tzd0cNrQDV3RRDt6Rxe0UKgLdaEu1IW68Ns+0B1dOE20oYUWGugTHegNhbpQF+pCXagLv+ED3dCGBrqjC6cNXdAdHWigOwp1oS7UhbpQF576QDsaaEMTbZwW2tDitHgU6kJdqAt1oS78hx3tnC5ocFpoog0NtKPB90JdqAt1oS7UhV/saEeDRwstFPSJghYaaKHBo1AX6kJdqAt14Sc72jktNNAFTbTQRANd0URBn3wv1IW6UBfqQl34sqMb37uiiRYaaKEDTXRDF54JdaEu1IW6UJcd7ZwGWmiggRaa6B0daEMDHWjwTKgLdaEu1IW67JwGWmihNzTRQhu6oYWCDjTR4JlQF+pCXagLdeHLQIvTQANNNNGFR1c00Q/0iQbfC3WhLtSFulCXwaOF3tBEg9PG6YYWOtDGa6Eu1IW6UBfqwpfFo4EmWmhDd7ShgTb0iX7wWqgLdaEu1IW68JOJ3tFEG7qjgQY6UNBEC01eC3WhLtSFulAXfjHQQpPThgaaaKCFBrrzWqgLdaEu1IW68GWid3SggRYaaKKghTa00IYmGjwT6kJdqAt1oS4LbWigwWmhgQ40UNCGFgq6o8UzoS7UhbpQF+r+BeXneQS8EjV4AAAAAElFTkSuQmCC\">"
      ],
      "text/plain": [
       "21×19 Matrix{Gray{Float64}}:\n",
       " 0.52625  0.52625  0.52625  0.52625  …  0.52625  0.52625  0.52625  0.52625\n",
       " 0.52625  0.52625  0.52625  0.52625     0.52625  0.52625  0.52625  0.52625\n",
       " 0.52625  0.52625  0.52625  0.52625     0.5275   0.52625  0.52625  0.52625\n",
       " 0.52625  0.52625  0.52625  0.52625     0.51875  0.5275   0.52625  0.52625\n",
       " 0.52625  0.52625  0.52625  0.52625     0.49     0.51875  0.5275   0.52625\n",
       " 0.52625  0.52625  0.52625  0.52625  …  0.485    0.49625  0.5075   0.5275\n",
       " 0.52625  0.52625  0.52625  0.52625     0.49     0.48625  0.5      0.51375\n",
       " 0.52625  0.52625  0.52625  0.52625     0.49     0.48375  0.5      0.51375\n",
       " 0.52625  0.52625  0.52625  0.525       0.48375  0.48375  0.5      0.51375\n",
       " 0.52625  0.52625  0.525    0.49625     0.48     0.48375  0.5      0.51375\n",
       " 0.52625  0.525    0.49625  0.4725   …  0.48     0.48375  0.5      0.51375\n",
       " 0.525    0.49625  0.47375  0.47125     0.48     0.48375  0.5      0.51375\n",
       " 0.49625  0.47125  0.4725   0.47375     0.48     0.48375  0.5      0.51375\n",
       " 0.47125  0.47125  0.475    0.46875     0.48     0.48375  0.5      0.51375\n",
       " 0.47125  0.475    0.4725   0.46875     0.48     0.48375  0.5      0.51375\n",
       " 0.47375  0.47625  0.47125  0.4675   …  0.48     0.48375  0.5      0.51375\n",
       " 0.475    0.4725   0.47     0.4675      0.48     0.48375  0.5      0.51375\n",
       " 0.475    0.47125  0.4625   0.49        0.48     0.48375  0.5      0.51375\n",
       " 0.47375  0.46875  0.4625   0.4925      0.48     0.48375  0.5      0.51375\n",
       " 0.47375  0.4625   0.48875  0.48125     0.48     0.48375  0.5      0.51375\n",
       " 0.4725   0.4725   0.495    0.48625  …  0.48     0.48375  0.5      0.51375"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Gray.(1 .- accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "476dbf34-3f9d-482e-a389-46b0325d6872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21×19 Matrix{Float64}:\n",
       " 0.47375  0.47375  0.47375  0.47375  …  0.47375  0.47375  0.47375  0.47375\n",
       " 0.47375  0.47375  0.47375  0.47375     0.47375  0.47375  0.47375  0.47375\n",
       " 0.47375  0.47375  0.47375  0.47375     0.4725   0.47375  0.47375  0.47375\n",
       " 0.47375  0.47375  0.47375  0.47375     0.48125  0.4725   0.47375  0.47375\n",
       " 0.47375  0.47375  0.47375  0.47375     0.51     0.48125  0.4725   0.47375\n",
       " 0.47375  0.47375  0.47375  0.47375  …  0.515    0.50375  0.4925   0.4725\n",
       " 0.47375  0.47375  0.47375  0.47375     0.51     0.51375  0.5      0.48625\n",
       " 0.47375  0.47375  0.47375  0.47375     0.51     0.51625  0.5      0.48625\n",
       " 0.47375  0.47375  0.47375  0.475       0.51625  0.51625  0.5      0.48625\n",
       " 0.47375  0.47375  0.475    0.50375     0.52     0.51625  0.5      0.48625\n",
       " 0.47375  0.475    0.50375  0.5275   …  0.52     0.51625  0.5      0.48625\n",
       " 0.475    0.50375  0.52625  0.52875     0.52     0.51625  0.5      0.48625\n",
       " 0.50375  0.52875  0.5275   0.52625     0.52     0.51625  0.5      0.48625\n",
       " 0.52875  0.52875  0.525    0.53125     0.52     0.51625  0.5      0.48625\n",
       " 0.52875  0.525    0.5275   0.53125     0.52     0.51625  0.5      0.48625\n",
       " 0.52625  0.52375  0.52875  0.5325   …  0.52     0.51625  0.5      0.48625\n",
       " 0.525    0.5275   0.53     0.5325      0.52     0.51625  0.5      0.48625\n",
       " 0.525    0.52875  0.5375   0.51        0.52     0.51625  0.5      0.48625\n",
       " 0.52625  0.53125  0.5375   0.5075      0.52     0.51625  0.5      0.48625\n",
       " 0.52625  0.5375   0.51125  0.51875     0.52     0.51625  0.5      0.48625\n",
       " 0.5275   0.5275   0.505    0.51375  …  0.52     0.51625  0.5      0.48625"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b7cb67f-dcaa-4fcd-a800-389d8b580625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CartesianIndex(10, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coordinate = argmax(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab4c8dd4-b872-413b-852c-4f0e578bdfa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8192.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = coordinate[1] |> i-> α[i] |> e-> 2.0^e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a16e8e86-49ee-41e9-b935-f8fce18f882e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001220703125"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "γ = coordinate[2] |> i-> β[i] |> e-> 2.0^e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd48bd8d-b921-4ac1-a68b-0932373677c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.3",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
