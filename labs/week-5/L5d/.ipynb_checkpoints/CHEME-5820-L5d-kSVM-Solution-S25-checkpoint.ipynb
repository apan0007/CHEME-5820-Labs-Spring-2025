{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32fe3c6d-09ea-4c0e-81f7-3c8454d05e77",
   "metadata": {},
   "source": [
    "# L5d: Kernelized Support Vector Machines (kSVMs)\n",
    "Fill me in\n",
    "\n",
    "### Theory: Support Vector Machine (SVM)\n",
    "Suppose, we have dataset $\\mathcal{D} = \\{(\\hat{\\mathbf{x}}_{i}, y_{i}) \\mid i = 1,2,\\dots,n\\}$, where $\\hat{\\mathbf{x}}_i \\in \\mathbb{R}^p$ is an _augmented_ feature vector ($m$ features with additional `1` to model the bias on the end of the vector) and $y_i \\in \\{-1, 1\\}$ is the corresponding class label.\n",
    "* __Objective__: the goal of an SVM is to find the hyperplane $\\mathcal{H}(\\hat{\\mathbf{x}}) = \\{\\hat{\\mathbf{x}} \\mid \\left<\\hat{\\mathbf{x}},\\theta\\right> = 0\\}$ that separates the data points into two classes (those points above the hyperplane, and those points below the hyperplane), \n",
    "where $\\theta \\in \\mathbb{R}^{p}$ ($p=m+1$) is the normal vector to the hyperplane, or alternatively, the parameters of the model that we need to estimate.\n",
    "* __Why another method__? Support vector machines (SVMs) and other approaches, e.g., [the perceptron](https://en.wikipedia.org/wiki/Perceptron) differ primarily in their optimization objectives and training methods: while a [perceptron](https://en.wikipedia.org/wiki/Perceptron) can find _a hyperplane_ that separates classes, SVMs seek to find the _best hyperplane_ in the sense that the _margin_ between classes is maximized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ef21be-0201-4515-89bb-ba8112541d8e",
   "metadata": {},
   "source": [
    "## Task 1: Setup, Data, and Prerequisites\n",
    "We set up the computational environment by including the `Include.jl` file, loading any needed resources, such as sample datasets, and setting up any required constants. \n",
    "* The `Include.jl` file also loads external packages, various functions that we will use in the exercise, and custom types to model the components of our problem. It checks for a `Manifest.toml` file; if it finds one, packages are loaded. Other packages are downloaded and then loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "070be284-e10c-4c83-90bb-4cab39d97ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Include.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be56e42-f2dd-4da0-a8df-b1bae63621eb",
   "metadata": {},
   "source": [
    "### Data\n",
    "First, let's set the number of samples `N` that we have in the data set $\\mathcal{D}$, and the number of feature variables `n`, i.e., the number of elements of $\\mathbf{x}\\in\\mathbb{R}^{n}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb75e970-d8fd-467e-8793-4122fce566ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_points_per_label = 1000; # number of samples per label (2 x this is the total)\n",
    "number_of_training_examples = 1200; # pick this many training examples at random\n",
    "number_of_features = 6; # we have this many features\n",
    "ϵ = 0.80; # threshold of random NLS points to accept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542a6e99-99b0-44c6-9266-8c473960f782",
   "metadata": {},
   "source": [
    "Let's set up the color dictionary to visualize the classification datasets. The keys of the `my_color_dictionary::Dict Int64, RGB` dictionary class labels, i.e., $ y\\in\\{1,-1\\}$ while the values are the colors mapped to that label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8bf8d35-4f28-448e-aa49-4fa6ae812b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_color_dictionary = Dict{Int64,RGB}();\n",
    "my_color_dictionary[1] = colorant\"#03045e\"; # color for Label = 1\n",
    "my_color_dictionary[-1] = colorant\"#e36414\"; # color for Label = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668d9e68-3fde-4b32-bf85-b34be4f508d3",
   "metadata": {},
   "source": [
    "Next, let's write a code to generate a [non-lineraly separable data set](https://en.wikipedia.org/wiki/Linear_separability) $\\mathcal{D}$ that we'll use to train (and test) our [kSVM](https://en.wikipedia.org/wiki/Support_vector_machine#Nonlinear_kernels).\n",
    "* To do this, let's generate random points and check the $y\\cdot\\left(\\mathbf{w}^{T}\\cdot\\mathbf{x}\\right) > 0$ condition. If this condition is true, we `accept` that data; otherwise, we `reject` the data. If we `accept` the data, we store it in the `dataset` variable, a [Set](https://docs.julialang.org/en/v1/base/collections/#Base.Set) holding a [NamedTuple type](https://docs.julialang.org/en/v1/base/base/#Core.NamedTuple), which is a mix of a tuple and a dictionary.\n",
    "* We keep iterating the loop until the number of elements of the `dataset` is greater than or equal to the desired number of test points specified in the `N` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4099b294-ce46-414c-98b4-69ea85e3743a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length(dataset) = 2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2000×7 Matrix{Float64}:\n",
       " -0.542757    -0.884737    0.221286   -1.14844   -1.5833     -1.72572    -1.0\n",
       "  1.80233      0.970726   -1.21554    -0.308287   0.978796    0.377945   -1.0\n",
       " -0.816177     1.65023     0.79423    -0.294944  -0.682418    1.00195    -1.0\n",
       "  0.242839    -0.250917   -0.837776   -1.50873    0.477805   -1.94528    -1.0\n",
       " -0.756158     0.560617   -0.0252681  -1.41705    0.820107   -0.277709   -1.0\n",
       "  2.14944      0.459069   -0.663656   -1.118      1.18082    -0.490702    1.0\n",
       " -0.131028    -1.06839     0.191483    0.141675  -1.02572    -0.0579715   1.0\n",
       "  0.4256      -2.16572     0.77048     0.898966   0.412077   -1.16296     1.0\n",
       "  1.22969     -0.0187894   1.45387     0.846223  -1.0572      0.544767    1.0\n",
       " -0.9305       0.65369    -0.231554    0.218396   1.16938    -0.100317   -1.0\n",
       "  0.00772959  -0.583314   -2.07893    -1.71707   -0.176019   -0.993017    1.0\n",
       " -0.208054     0.133715    0.281434    0.500232   2.00872    -1.48785     1.0\n",
       "  0.0556555   -0.0192339   1.20746     0.819229   0.727563   -0.647877   -1.0\n",
       "  ⋮                                                           ⋮          \n",
       "  1.12762     -0.333526    0.61241     0.405404   0.471691    1.0997      1.0\n",
       "  1.8138      -1.04174    -1.69007    -0.035822  -0.4772     -0.558376   -1.0\n",
       " -0.119839     0.590805    1.07888    -0.634078  -1.23069    -1.59484    -1.0\n",
       "  1.47267      1.61481     0.452258   -0.580013  -0.287554   -0.330664   -1.0\n",
       "  2.34148      1.02377    -0.0157932  -1.40394    0.536112    2.07235     1.0\n",
       " -0.994789     1.43647    -0.729099    2.02185    0.479203   -0.346091    1.0\n",
       " -1.20779      0.448449    1.84736    -0.753657  -1.89841     1.45417     1.0\n",
       "  0.403209     0.464935    0.259028   -0.901966  -0.689118   -0.745576    1.0\n",
       " -0.369659    -1.39175    -0.0802126  -0.607709  -0.757817   -0.0464857   1.0\n",
       " -0.331776    -1.54391     1.10178     0.503032  -0.648838    0.582551    1.0\n",
       "  0.800163     1.40685    -0.453004    1.11535   -0.0168664  -0.393899   -1.0\n",
       " -0.426892     0.21207     0.0802667  -0.117142   1.39158    -0.311631   -1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = let\n",
    "\n",
    "    # initialize \n",
    "    dataset = Set{@NamedTuple{x::Array{Float64,1},y::Int64}}();\n",
    "    w = randn(number_of_features+1); # tmp parameters that we use to make data\n",
    "    classes = [-1,1]; # binary classes\n",
    "    # d = Categorical([0.51,0.49]);\n",
    "    should_keep_looping = true;\n",
    "    \n",
    "    while (should_keep_looping == true)\n",
    "    \n",
    "        x = randn(number_of_features) |> x -> push!(x,1); # generate a random augmented feature vector\n",
    "        y = 1;\n",
    "        \n",
    "        # check -\n",
    "        if (y*(sum(w.*x)) ≥ 0.0)\n",
    "            data = (x = x, y = y); # use a tuple, and set to enforce unique\n",
    "            push!(dataset,data);\n",
    "        else\n",
    "            # fails, should we keep this data?\n",
    "            if (rand() ≥ ϵ)\n",
    "                data = (x = x, y = y); # use a tuple, and set to enforce unique\n",
    "                push!(dataset,data);\n",
    "            end\n",
    "        end\n",
    "    \n",
    "        # if we have enough examples, stop iterating\n",
    "        if (length(dataset) >= number_of_points_per_label)\n",
    "            should_keep_looping = false\n",
    "        end\n",
    "    end\n",
    "\n",
    "    should_keep_looping = true;\n",
    "    while (should_keep_looping == true)\n",
    "    \n",
    "        x = randn(number_of_features) |> x -> push!(x,1); # generate a random augmented feature vector\n",
    "        y = -1;\n",
    "        \n",
    "        # check -\n",
    "        if (y*(sum(w.*x)) ≥ 0.0)\n",
    "            data = (x = x, y = y); # use a tuple, and set to enforce unique\n",
    "            push!(dataset,data);\n",
    "        else\n",
    "            # fails, should we keep this data?\n",
    "            if (rand() ≥ ϵ)\n",
    "                data = (x = x, y = y); # use a tuple, and set to enforce unique\n",
    "                push!(dataset,data);\n",
    "            end\n",
    "        end\n",
    "    \n",
    "        # if we have enough examples, stop iterating\n",
    "        if (length(dataset) >= 2*number_of_points_per_label)\n",
    "            should_keep_looping = false\n",
    "        end\n",
    "    end\n",
    "\n",
    "    @show length(dataset)\n",
    "\n",
    "    # ok, so let's put this data into a matrix -\n",
    "    D = Array{Float64,2}(undef, 2*number_of_points_per_label, number_of_features+1);\n",
    "    for i ∈ 1:2*number_of_points_per_label\n",
    "        example = pop!(dataset);\n",
    "        feature = example.x;\n",
    "        label = example.y;\n",
    "        for j ∈ 1:number_of_features\n",
    "            D[i,j] = feature[j];\n",
    "        end\n",
    "        D[i,end] = label;\n",
    "    end\n",
    "    D\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263a91da-873b-4d35-b339-4848d555c236",
   "metadata": {},
   "source": [
    "### Visualize dataset `D`\n",
    "`Unhide` the code block below to see how we plotted the dataset `D`, which contains two continuous features and a label. The color indicates the label.\n",
    "* __Summary__: We will get a different pattern of $\\pm{1}$ labels depending on radius $r_{1}$ and $r_{2}$ values, and any additional labeling logic $L(x,y)$ we used. The dark blue dots represent label `1`, while the orange data represents label `1`. Our classifier should be able to learn the mapping between the features and the labels for linearly separable datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7302bcbf-7bce-4d74-9619-5889a0ca8af4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mVisualization disabled\n"
     ]
    }
   ],
   "source": [
    "let\n",
    "\n",
    "    dataset = D; # what dataset am I looking at?\n",
    "    number_of_points_to_plot = size(dataset,1);\n",
    "    if (number_of_features > 2)\n",
    "        @info \"Visualization disabled\"\n",
    "    else\n",
    "        p = plot(bg=\"gray95\", background_color_outside=\"white\", framestyle = :box, fg_legend = :transparent); # make an empty plot\n",
    "    \n",
    "        # plot label = 1\n",
    "        testlabel = 1;\n",
    "        i = findfirst(label -> label == testlabel,  dataset[:,3])\n",
    "        c = my_color_dictionary[testlabel]\n",
    "        scatter!([dataset[i,1]], [dataset[i,2]], label=\"Label: $(testlabel)\", c=c)\n",
    "    \n",
    "        # plot label = -1\n",
    "        testlabel = -1;\n",
    "        i = findfirst(label -> label == testlabel,  dataset[:,3])\n",
    "        c = my_color_dictionary[testlabel]\n",
    "        scatter!([dataset[i,1]], [dataset[i,2]], label=\"Label: $(testlabel)\", c=c)\n",
    "    \n",
    "        # plot all points\n",
    "        for i ∈ 1:number_of_points_to_plot\n",
    "            label = dataset[i,3]; # label\n",
    "            c = my_color_dictionary[label]\n",
    "            scatter!([dataset[i, 1]], [dataset[i, 2]], label=\"\", mec=:navy, c=c)\n",
    "        end\n",
    "        \n",
    "        xlabel!(\"Feature 1 (AU)\", fontsize=18);\n",
    "        ylabel!(\"Feature 2 (AU)\", fontsize=18);\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a138b06c-0106-4e55-b68e-bdd561076c64",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "training, test = let\n",
    "\n",
    "    number_of_features = size(D,2); # number of cols of housing data\n",
    "    number_of_examples = size(D,1); # number of rows of housing data\n",
    "    full_index_set = range(1,stop=number_of_examples,step=1) |> collect |> Set;\n",
    "    \n",
    "    # build index sets for training and testing\n",
    "    training_index_set = Set{Int64}();\n",
    "    should_stop_loop = false;\n",
    "    while (should_stop_loop == false)\n",
    "        i = rand(1:number_of_examples);\n",
    "        push!(training_index_set,i);\n",
    "\n",
    "        if (length(training_index_set) == number_of_training_examples)\n",
    "            should_stop_loop = true;\n",
    "        end\n",
    "    end\n",
    "    test_index_set = setdiff(full_index_set,training_index_set);\n",
    "\n",
    "    # build the test and train datasets -\n",
    "    training = D[training_index_set |> collect,:];\n",
    "    test = D[test_index_set |> collect,:];\n",
    "\n",
    "    # return\n",
    "    training, test\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383ff19e-5964-4fc5-90c8-0f0577ba06bc",
   "metadata": {},
   "source": [
    "## Task 2: Classification using an SVM\n",
    "In this task, we [use the SVM implementation exported by the `LIBSVM.jl` package](https://github.com/JuliaML/LIBSVM.jl) to classify the [banknote authentication dataset from the UCI archive](https://archive.ics.uci.edu/dataset/267/banknote+authentication). In particular, we use the `training` dataset to estimate the unknown model parameters $\\theta$ [using the `svmtrain(...)` method](https://github.com/JuliaML/LIBSVM.jl/blob/master/src/LIBSVM.jl). \n",
    "* The [`svmtrain(...)` method](https://github.com/JuliaML/LIBSVM.jl/blob/master/src/LIBSVM.jl) takes a matrix of feature vectors where augmented training examples matrix $\\hat{\\mathbf{X}}^{\\top}$, i.e., the examples are on the columns and the features are the rows, and a label vector $\\mathbf{y}\\in\\left\\{-1,1\\right\\}$.\n",
    "* The [`svmtrain(...)` method](https://github.com/JuliaML/LIBSVM.jl/blob/master/src/LIBSVM.jl) returns a [model instance](https://github.com/JuliaML/LIBSVM.jl/blob/master/src/LIBSVM.jl) that holds the trained data and a bunch of other data associated with the problem.\n",
    "* __Hmmm__: One of the (super) interesting optional arguments [the `svmtrain(...)` method](https://github.com/JuliaML/LIBSVM.jl/blob/master/src/LIBSVM.jl) is the `kernel` argument. Check out the documentation to see what kernels are supported! Wow! we get [kernelized SVM capability](https://en.wikipedia.org/wiki/Support_vector_machine#Nonlinear_kernels) right out of the box. Buy versus build, 99% buy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d9c5e427-8dce-47b2-a1d3-51fd7bcfbbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*\n",
      "optimization finished, #iter = 162\n",
      "nu = 0.657494\n",
      "obj = -111.519911, rho = -0.475122\n",
      "nSV = 144, nBSV = 119\n",
      "Total nSV = 144\n"
     ]
    }
   ],
   "source": [
    "model = let\n",
    "\n",
    "    # Setup the data that we are using\n",
    "    D = training; # what dataset are we looking at?\n",
    "    number_of_examples = size(D,1); # how many rows?\n",
    "    X = [D[:,1:end-1] ones(number_of_examples)] |> transpose |> Matrix; # augmented features (arranged as m x n)\n",
    "    y = D[:,end]; # label\n",
    "\n",
    "    # Train the data -\n",
    "    model = svmtrain(X, y, kernel=LIBSVM.Kernel.RadialBasis, verbose = true); # we are using the LIBSVM\n",
    "\n",
    "    # return\n",
    "    model\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a40341-4c8c-4511-8a6f-814182d10735",
   "metadata": {},
   "source": [
    "__Inference__: Now that we have parameters estimated from the `training` data, we can use those parameters on the `test` dataset to see how well the model can differentiate between an actual banknote and a forgery on data it has never seen. We run the classification operation on the (unseen) test data [using the `svmpredict(...)` method](https://github.com/JuliaML/LIBSVM.jl/blob/master/src/LIBSVM.jl). \n",
    "* The [`svmpredict(...)` method](https://github.com/JuliaML/LIBSVM.jl/blob/master/src/LIBSVM.jl) returns the predicted label which we store in the `ŷ::Array{Int64,1}` array. We store the actual (correct) label in the `y::Array{Int64,1}` vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b42d7969-9bfa-4f4f-98fb-ea9443c11eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ,y,d = let\n",
    "\n",
    "     # Setup the data that we are using\n",
    "    D = test; # what dataset are we looking at?\n",
    "    number_of_examples = size(D,1); # how many rows?\n",
    "    X = [D[:,1:end-1] ones(number_of_examples)] |> transpose |> Matrix; # features (arranged as m x n)\n",
    "    y = D[:,end]; # label\n",
    "    \n",
    "    # Test model on the other half of the data.\n",
    "    ŷ, decision_values = svmpredict(model, X);\n",
    "\n",
    "    # return -\n",
    "    ŷ,y,decision_values\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efea4db2-103c-4e8f-b858-96dd1fb66db5",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "The confusion matrix is a $2\\times{2}$ matrix that contains four entries: true positive (TP), false positive (FP), true negative (TN), and false negative (FN). [Click me for a confusion matrix schematic!](https://github.com/varnerlab/CHEME-5820-Labs-Spring-2025/blob/main/labs/week-3/L3b/figs/Fig-BinaryConfusionMatrix.pdf). Let's compute these four values [using the `confusion(...)` method](src/Compute.jl) and store them in the `CM::Array{Int64,2}` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d57ea7ec-1fd8-41cf-9d4d-e85768e10d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Matrix{Int64}:\n",
       " 535  366\n",
       " 120  779"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CM = confusion(y, ŷ) # call with the SVM values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "52750870-9c80-4e72-8dfa-6761ba710171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction correct: 0.73 Fraction incorrect 0.27\n"
     ]
    }
   ],
   "source": [
    "number_of_test_points = length(y);\n",
    "correct_prediction_perceptron = CM[1,1] + CM[2,2];\n",
    "(correct_prediction_perceptron/number_of_test_points) |> f-> println(\"Fraction correct: $(f) Fraction incorrect $(1-f)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0e2021-5686-473e-8f6b-a4508bb14a31",
   "metadata": {},
   "source": [
    "## Task 3: Implement Grid Search for Optimal Hyperparameters\n",
    "Fill me in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "916a6ac0-a18f-4b0e-9aa5-dda1e0fa7dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "error, α, β = let\n",
    "    \n",
    "    # Training data setup -\n",
    "    D₁ = training; # what dataset are we looking at?\n",
    "    number_of_training_examples = size(D₁,1); # how many rows?\n",
    "    X₁ = [D₁[:,1:end-1] ones(number_of_training_examples)] |> transpose |> Matrix; # augmented features (arranged as m x n)\n",
    "    y₁ = D₁[:,end]; # label\n",
    "\n",
    "    # Test data setup -\n",
    "    D₂ = test; # what dataset are we looking at?\n",
    "    number_of_test_examples = size(D₂,1); # how many rows?\n",
    "    X₂ = [D₂[:,1:end-1] ones(number_of_test_examples)] |> transpose |> Matrix; # features (arranged as m x n)\n",
    "    y₂ = D₂[:,end]; # label\n",
    "    \n",
    "    α = range(-5,stop = 15, step=2) |> collect; # exponent for C -\n",
    "    β = range(-15,stop = 3, step=2) |> collect; # exponent for γ -\n",
    "    number_of_points_C = length(α);\n",
    "    number_of_points_gamma = length(β);\n",
    "    error = Array{Float64,2}(undef, number_of_points_C, number_of_points_gamma);\n",
    "    \n",
    "    for i ∈ eachindex(α)\n",
    "        C = 2.0^α[i];\n",
    "        for j ∈ eachindex(β)\n",
    "            γ = 2.0^β[j];\n",
    "\n",
    "            # train model -\n",
    "            ŷ₂,_ = svmtrain(X₁, y₁, kernel=LIBSVM.Kernel.RadialBasis, \n",
    "                verbose = false, cost = C, gamma = γ) |> model -> svmpredict(model,X₂);\n",
    "\n",
    "            # how many mistakes?\n",
    "            CM = confusion(y₂, ŷ₂);\n",
    "            correct_prediction_perceptron = CM[1,1] + CM[2,2];\n",
    "            error[i,j] = (correct_prediction_perceptron/number_of_test_examples);\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    error, α, β\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bfccd256-16f9-4a9f-a649-f25f161fdb53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGQAAABuCAAAAAD0EunuAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAASpJREFUaAW9wbFtkwEABtGzczbKAHSwK8shhPdgAmr7p/pojKKk4N7zG/+fBCQgAQlIQAISkIAE5MO+M6/MZ+YrzyQgAQlIQAISkIAEJCDv9IO5Mgfzm7kzL4wEJCABCUhAAhKQgATkTT+ZC/Ngzjz7xXxhJCABCUhAAhKQgAQkIP9wY16Ygzkzr4zMwTMJSEACEpCABCQgAQnIXzfmxLM7c2XuzJW3SEACEpCABCQgAQlIwBtzYk7MnZG5MFfeRwISkIAEJCABCUhAAp6YM3Pn2QvzibkwD94iAQlIQAISkIAEJCABz8yDORiZg3kwJ95HAhKQgAQkIAEJSEACHszBszNzYeSjJCABCUhAAhKQgAQk4MEcjDy7MGc+SgISkIAEJCABCUhAAn8Ag1Ubam3q8eIAAAAASUVORK5CYII=",
      "text/html": [
       "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAGQAAABuCAAAAAD0EunuAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAASpJREFUaAW9wbFtkwEABtGzczbKAHSwK8shhPdgAmr7p/pojKKk4N7zG/+fBCQgAQlIQAISkIAE5MO+M6/MZ+YrzyQgAQlIQAISkIAEJCDv9IO5Mgfzm7kzL4wEJCABCUhAAhKQgATkTT+ZC/Ngzjz7xXxhJCABCUhAAhKQgAQkIP9wY16Ygzkzr4zMwTMJSEACEpCABCQgAQnIXzfmxLM7c2XuzJW3SEACEpCABCQgAQlIwBtzYk7MnZG5MFfeRwISkIAEJCABCUhAAp6YM3Pn2QvzibkwD94iAQlIQAISkIAEJCABz8yDORiZg3kwJ95HAhKQgAQkIAEJSEACHszBszNzYeSjJCABCUhAAhKQgAQk4MEcjDy7MGc+SgISkIAEJCABCUhAAn8Ag1Ubam3q8eIAAAAASUVORK5C\">"
      ],
      "text/plain": [
       "11×10 Matrix{Gray{Float64}}:\n",
       " 0.500556  0.500556  0.500556  0.500556  …  0.500556  0.500556  0.500556\n",
       " 0.500556  0.500556  0.500556  0.500556     0.387222  0.500556  0.500556\n",
       " 0.500556  0.500556  0.500556  0.500556     0.335556  0.491667  0.500556\n",
       " 0.500556  0.500556  0.500556  0.287778     0.336111  0.382778  0.492222\n",
       " 0.500556  0.500556  0.289444  0.297222     0.361111  0.381667  0.492222\n",
       " 0.500556  0.29      0.295     0.298889  …  0.368889  0.381667  0.492222\n",
       " 0.290556  0.293333  0.298889  0.286667     0.368889  0.381667  0.492222\n",
       " 0.293889  0.300556  0.291111  0.285        0.368889  0.381667  0.492222\n",
       " 0.302222  0.295556  0.287222  0.303333     0.368889  0.381667  0.492222\n",
       " 0.296667  0.29      0.286111  0.311111     0.368889  0.381667  0.492222\n",
       " 0.293889  0.285556  0.301667  0.308889  …  0.368889  0.381667  0.492222"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Gray.(1 .- error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "476dbf34-3f9d-482e-a389-46b0325d6872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11×10 Matrix{Float64}:\n",
       " 0.499444  0.499444  0.499444  0.499444  …  0.499444  0.499444  0.499444\n",
       " 0.499444  0.499444  0.499444  0.499444     0.612778  0.499444  0.499444\n",
       " 0.499444  0.499444  0.499444  0.499444     0.664444  0.508333  0.499444\n",
       " 0.499444  0.499444  0.499444  0.712222     0.663889  0.617222  0.507778\n",
       " 0.499444  0.499444  0.710556  0.702778     0.638889  0.618333  0.507778\n",
       " 0.499444  0.71      0.705     0.701111  …  0.631111  0.618333  0.507778\n",
       " 0.709444  0.706667  0.701111  0.713333     0.631111  0.618333  0.507778\n",
       " 0.706111  0.699444  0.708889  0.715        0.631111  0.618333  0.507778\n",
       " 0.697778  0.704444  0.712778  0.696667     0.631111  0.618333  0.507778\n",
       " 0.703333  0.71      0.713889  0.688889     0.631111  0.618333  0.507778\n",
       " 0.706111  0.714444  0.698333  0.691111  …  0.631111  0.618333  0.507778"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1b7cb67f-dcaa-4fcd-a800-389d8b580625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CartesianIndex(2, 6)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coordinate = argmax(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ab4c8dd4-b872-413b-852c-4f0e578bdfa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.125"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = coordinate[1] |> i-> α[i] |> e-> 2.0^e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a16e8e86-49ee-41e9-b935-f8fce18f882e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03125"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "γ = coordinate[2] |> i-> β[i] |> e-> 2.0^e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd48bd8d-b921-4ac1-a68b-0932373677c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.3",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
