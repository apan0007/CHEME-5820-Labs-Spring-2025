{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32fe3c6d-09ea-4c0e-81f7-3c8454d05e77",
   "metadata": {},
   "source": [
    "# L5d: Kernelized Support Vector Machines (kSVMs)\n",
    "Fill me in\n",
    "\n",
    "### Theory: Support Vector Machine (SVM)\n",
    "Suppose, we have dataset $\\mathcal{D} = \\{(\\hat{\\mathbf{x}}_{i}, y_{i}) \\mid i = 1,2,\\dots,n\\}$, where $\\hat{\\mathbf{x}}_i \\in \\mathbb{R}^p$ is an _augmented_ feature vector ($m$ features with additional `1` to model the bias on the end of the vector) and $y_i \\in \\{-1, 1\\}$ is the corresponding class label.\n",
    "* __Objective__: the goal of an SVM is to find the hyperplane $\\mathcal{H}(\\hat{\\mathbf{x}}) = \\{\\hat{\\mathbf{x}} \\mid \\left<\\hat{\\mathbf{x}},\\theta\\right> = 0\\}$ that separates the data points into two classes (those points above the hyperplane, and those points below the hyperplane), \n",
    "where $\\theta \\in \\mathbb{R}^{p}$ ($p=m+1$) is the normal vector to the hyperplane, or alternatively, the parameters of the model that we need to estimate.\n",
    "* __Why another method__? Support vector machines (SVMs) and other approaches, e.g., [the perceptron](https://en.wikipedia.org/wiki/Perceptron) differ primarily in their optimization objectives and training methods: while a [perceptron](https://en.wikipedia.org/wiki/Perceptron) can find _a hyperplane_ that separates classes, SVMs seek to find the _best hyperplane_ in the sense that the _margin_ between classes is maximized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ef21be-0201-4515-89bb-ba8112541d8e",
   "metadata": {},
   "source": [
    "## Task 1: Setup, Data, and Prerequisites\n",
    "We set up the computational environment by including the `Include.jl` file, loading any needed resources, such as sample datasets, and setting up any required constants. \n",
    "* The `Include.jl` file also loads external packages, various functions that we will use in the exercise, and custom types to model the components of our problem. It checks for a `Manifest.toml` file; if it finds one, packages are loaded. Other packages are downloaded and then loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "070be284-e10c-4c83-90bb-4cab39d97ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Include.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be56e42-f2dd-4da0-a8df-b1bae63621eb",
   "metadata": {},
   "source": [
    "### Data\n",
    "First, let's set the number of samples `N` that we have in the data set $\\mathcal{D}$, and the number of feature variables `n`, i.e., the number of elements of $\\mathbf{x}\\in\\mathbb{R}^{n}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb75e970-d8fd-467e-8793-4122fce566ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_points_per_label = 1000; # number of samples per label (2 x this is the total)\n",
    "number_of_training_examples = 1200; # pick this many training examples at random\n",
    "number_of_features = 6; # we have this many features\n",
    "ϵ = 0.80; # threshold of random NLS points to accept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542a6e99-99b0-44c6-9266-8c473960f782",
   "metadata": {},
   "source": [
    "Let's set up the color dictionary to visualize the classification datasets. The keys of the `my_color_dictionary::Dict Int64, RGB` dictionary class labels, i.e., $ y\\in\\{1,-1\\}$ while the values are the colors mapped to that label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8bf8d35-4f28-448e-aa49-4fa6ae812b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_color_dictionary = Dict{Int64,RGB}();\n",
    "my_color_dictionary[1] = colorant\"#03045e\"; # color for Label = 1\n",
    "my_color_dictionary[-1] = colorant\"#e36414\"; # color for Label = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668d9e68-3fde-4b32-bf85-b34be4f508d3",
   "metadata": {},
   "source": [
    "Next, let's write a code to generate a [non-lineraly separable data set](https://en.wikipedia.org/wiki/Linear_separability) $\\mathcal{D}$ that we'll use to train (and test) our [kSVM](https://en.wikipedia.org/wiki/Support_vector_machine#Nonlinear_kernels).\n",
    "* To do this, let's generate random points and check the $y\\cdot\\left(\\mathbf{w}^{T}\\cdot\\mathbf{x}\\right) > 0$ condition. If this condition is true, we `accept` that data; otherwise, we `reject` the data. If we `accept` the data, we store it in the `dataset` variable, a [Set](https://docs.julialang.org/en/v1/base/collections/#Base.Set) holding a [NamedTuple type](https://docs.julialang.org/en/v1/base/base/#Core.NamedTuple), which is a mix of a tuple and a dictionary.\n",
    "* We keep iterating the loop until the number of elements of the `dataset` is greater than or equal to the desired number of test points specified in the `N` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4099b294-ce46-414c-98b4-69ea85e3743a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length(dataset) = 2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2000×7 Matrix{Float64}:\n",
       "  0.366608   -1.8518      0.0453876   …   0.928255     0.973175  -1.0\n",
       "  0.569287    0.16533    -0.699301       -1.19719      0.788789  -1.0\n",
       " -0.0333475  -0.301728   -0.84503         1.56218     -1.57878    1.0\n",
       "  0.462384    0.103008    0.00122587     -1.58468      0.18515   -1.0\n",
       "  0.57401     0.33464     2.618          -0.0346364    0.563698  -1.0\n",
       "  1.88476     0.117542    0.467889    …   1.04374     -0.625174   1.0\n",
       " -1.52623     0.434367    1.30079         1.12978      1.16804    1.0\n",
       " -0.948837    1.1789     -0.565499        0.331576     2.24       1.0\n",
       "  0.960255    0.562334    0.359629        1.38792     -0.363901  -1.0\n",
       "  0.731789   -0.0437454   1.89726         0.17276     -0.216853  -1.0\n",
       " -0.260946   -1.23813     1.5371      …   0.472599    -0.361515   1.0\n",
       " -1.60839     0.781026    0.436826        0.00838269  -0.402502   1.0\n",
       " -1.52778     1.50594     0.280307       -1.62057     -0.549524  -1.0\n",
       "  ⋮                                   ⋱                ⋮         \n",
       " -0.511972    2.84322    -2.02733         0.532108     1.43748    1.0\n",
       "  1.0368      0.771388   -1.4477          0.571395     0.201056  -1.0\n",
       " -0.23676     0.0543224   0.950618    …   0.569413    -1.82087   -1.0\n",
       "  0.19508    -0.327176   -0.646826       -0.937134    -0.728688  -1.0\n",
       " -0.413523   -0.209422    1.17773        -0.525183    -1.44844   -1.0\n",
       "  0.430855    0.26543     0.0147921      -0.207659    -0.237813   1.0\n",
       " -0.889515   -0.601874    0.657183        1.27156     -1.97171    1.0\n",
       "  0.140491    0.334489    0.896741    …  -1.60175     -0.943502  -1.0\n",
       " -1.15683    -1.58247     0.516876        1.57689      0.362505   1.0\n",
       " -0.506496   -0.769861   -1.94085        -1.56174      0.374293  -1.0\n",
       " -0.319838    2.0664      0.192061       -2.47025      0.961269  -1.0\n",
       "  0.283942    0.609527    0.317974        0.490618    -0.370419  -1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = let\n",
    "\n",
    "    # initialize \n",
    "    dataset = Set{@NamedTuple{x::Array{Float64,1},y::Int64}}();\n",
    "    w = randn(number_of_features+1); # tmp parameters that we use to make data\n",
    "    classes = [-1,1]; # binary classes\n",
    "    # d = Categorical([0.51,0.49]);\n",
    "    should_keep_looping = true;\n",
    "    \n",
    "    while (should_keep_looping == true)\n",
    "    \n",
    "        x = randn(number_of_features) |> x -> push!(x,1); # generate a random augmented feature vector\n",
    "        y = 1;\n",
    "        \n",
    "        # check -\n",
    "        if (y*(sum(w.*x)) ≥ 0.0)\n",
    "            data = (x = x, y = y); # use a tuple, and set to enforce unique\n",
    "            push!(dataset,data);\n",
    "        else\n",
    "            # fails, should we keep this data?\n",
    "            if (rand() ≥ ϵ)\n",
    "                data = (x = x, y = y); # use a tuple, and set to enforce unique\n",
    "                push!(dataset,data);\n",
    "            end\n",
    "        end\n",
    "    \n",
    "        # if we have enough examples, stop iterating\n",
    "        if (length(dataset) >= number_of_points_per_label)\n",
    "            should_keep_looping = false\n",
    "        end\n",
    "    end\n",
    "\n",
    "    should_keep_looping = true;\n",
    "    while (should_keep_looping == true)\n",
    "    \n",
    "        x = randn(number_of_features) |> x -> push!(x,1); # generate a random augmented feature vector\n",
    "        y = -1;\n",
    "        \n",
    "        # check -\n",
    "        if (y*(sum(w.*x)) ≥ 0.0)\n",
    "            data = (x = x, y = y); # use a tuple, and set to enforce unique\n",
    "            push!(dataset,data);\n",
    "        else\n",
    "            # fails, should we keep this data?\n",
    "            if (rand() ≥ ϵ)\n",
    "                data = (x = x, y = y); # use a tuple, and set to enforce unique\n",
    "                push!(dataset,data);\n",
    "            end\n",
    "        end\n",
    "    \n",
    "        # if we have enough examples, stop iterating\n",
    "        if (length(dataset) >= 2*number_of_points_per_label)\n",
    "            should_keep_looping = false\n",
    "        end\n",
    "    end\n",
    "\n",
    "    @show length(dataset)\n",
    "\n",
    "    # ok, so let's put this data into a matrix -\n",
    "    D = Array{Float64,2}(undef, 2*number_of_points_per_label, number_of_features+1);\n",
    "    for i ∈ 1:2*number_of_points_per_label\n",
    "        example = pop!(dataset);\n",
    "        feature = example.x;\n",
    "        label = example.y;\n",
    "        for j ∈ 1:number_of_features\n",
    "            D[i,j] = feature[j];\n",
    "        end\n",
    "        D[i,end] = label;\n",
    "    end\n",
    "    D\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263a91da-873b-4d35-b339-4848d555c236",
   "metadata": {},
   "source": [
    "### Visualize dataset `D`\n",
    "`Unhide` the code block below to see how we plotted the dataset `D`, which contains two continuous features and a label. The color indicates the label.\n",
    "* __Summary__: We will get a different pattern of $\\pm{1}$ labels depending on radius $r_{1}$ and $r_{2}$ values, and any additional labeling logic $L(x,y)$ we used. The dark blue dots represent label `1`, while the orange data represents label `1`. Our classifier should be able to learn the mapping between the features and the labels for linearly separable datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7302bcbf-7bce-4d74-9619-5889a0ca8af4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mVisualization disabled\n"
     ]
    }
   ],
   "source": [
    "let\n",
    "\n",
    "    dataset = D; # what dataset am I looking at?\n",
    "    number_of_points_to_plot = size(dataset,1);\n",
    "    if (number_of_features > 2)\n",
    "        @info \"Visualization disabled\"\n",
    "    else\n",
    "        p = plot(bg=\"gray95\", background_color_outside=\"white\", framestyle = :box, fg_legend = :transparent); # make an empty plot\n",
    "    \n",
    "        # plot label = 1\n",
    "        testlabel = 1;\n",
    "        i = findfirst(label -> label == testlabel,  dataset[:,3])\n",
    "        c = my_color_dictionary[testlabel]\n",
    "        scatter!([dataset[i,1]], [dataset[i,2]], label=\"Label: $(testlabel)\", c=c)\n",
    "    \n",
    "        # plot label = -1\n",
    "        testlabel = -1;\n",
    "        i = findfirst(label -> label == testlabel,  dataset[:,3])\n",
    "        c = my_color_dictionary[testlabel]\n",
    "        scatter!([dataset[i,1]], [dataset[i,2]], label=\"Label: $(testlabel)\", c=c)\n",
    "    \n",
    "        # plot all points\n",
    "        for i ∈ 1:number_of_points_to_plot\n",
    "            label = dataset[i,3]; # label\n",
    "            c = my_color_dictionary[label]\n",
    "            scatter!([dataset[i, 1]], [dataset[i, 2]], label=\"\", mec=:navy, c=c)\n",
    "        end\n",
    "        \n",
    "        xlabel!(\"Feature 1 (AU)\", fontsize=18);\n",
    "        ylabel!(\"Feature 2 (AU)\", fontsize=18);\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a138b06c-0106-4e55-b68e-bdd561076c64",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "training, test = let\n",
    "\n",
    "    number_of_features = size(D,2); # number of cols of housing data\n",
    "    number_of_examples = size(D,1); # number of rows of housing data\n",
    "    full_index_set = range(1,stop=number_of_examples,step=1) |> collect |> Set;\n",
    "    \n",
    "    # build index sets for training and testing\n",
    "    training_index_set = Set{Int64}();\n",
    "    should_stop_loop = false;\n",
    "    while (should_stop_loop == false)\n",
    "        i = rand(1:number_of_examples);\n",
    "        push!(training_index_set,i);\n",
    "\n",
    "        if (length(training_index_set) == number_of_training_examples)\n",
    "            should_stop_loop = true;\n",
    "        end\n",
    "    end\n",
    "    test_index_set = setdiff(full_index_set,training_index_set);\n",
    "\n",
    "    # build the test and train datasets -\n",
    "    training = D[training_index_set |> collect,:];\n",
    "    test = D[test_index_set |> collect,:];\n",
    "\n",
    "    # return\n",
    "    training, test\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383ff19e-5964-4fc5-90c8-0f0577ba06bc",
   "metadata": {},
   "source": [
    "## Task 2: Classification using an SVM\n",
    "In this task, we [use the SVM implementation exported by the `LIBSVM.jl` package](https://github.com/JuliaML/LIBSVM.jl) to classify the [banknote authentication dataset from the UCI archive](https://archive.ics.uci.edu/dataset/267/banknote+authentication). In particular, we use the `training` dataset to estimate the unknown model parameters $\\theta$ [using the `svmtrain(...)` method](https://github.com/JuliaML/LIBSVM.jl/blob/master/src/LIBSVM.jl). \n",
    "* The [`svmtrain(...)` method](https://github.com/JuliaML/LIBSVM.jl/blob/master/src/LIBSVM.jl) takes a matrix of feature vectors where augmented training examples matrix $\\hat{\\mathbf{X}}^{\\top}$, i.e., the examples are on the columns and the features are the rows, and a label vector $\\mathbf{y}\\in\\left\\{-1,1\\right\\}$.\n",
    "* The [`svmtrain(...)` method](https://github.com/JuliaML/LIBSVM.jl/blob/master/src/LIBSVM.jl) returns a [model instance](https://github.com/JuliaML/LIBSVM.jl/blob/master/src/LIBSVM.jl) that holds the trained data and a bunch of other data associated with the problem.\n",
    "* __Hmmm__: One of the (super) interesting optional arguments [the `svmtrain(...)` method](https://github.com/JuliaML/LIBSVM.jl/blob/master/src/LIBSVM.jl) is the `kernel` argument. Check out the documentation to see what kernels are supported! Wow! we get [kernelized SVM capability](https://en.wikipedia.org/wiki/Support_vector_machine#Nonlinear_kernels) right out of the box. Buy versus build, 99% buy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9c5e427-8dce-47b2-a1d3-51fd7bcfbbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*\n",
      "optimization finished, #iter = 936\n",
      "nu = 0.559723\n",
      "obj = -623.225616, rho = 0.103884\n",
      "nSV = 713, nBSV = 627\n",
      "Total nSV = 713\n"
     ]
    }
   ],
   "source": [
    "model = let\n",
    "\n",
    "    # Setup the data that we are using\n",
    "    D = training; # what dataset are we looking at?\n",
    "    number_of_examples = size(D,1); # how many rows?\n",
    "    X = [D[:,1:end-1] ones(number_of_examples)] |> transpose |> Matrix; # augmented features (arranged as m x n)\n",
    "    y = D[:,end]; # label\n",
    "\n",
    "    # Train the data -\n",
    "    model = svmtrain(X, y, kernel=LIBSVM.Kernel.RadialBasis, verbose = true); # we are using the LIBSVM\n",
    "\n",
    "    # return\n",
    "    model\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a40341-4c8c-4511-8a6f-814182d10735",
   "metadata": {},
   "source": [
    "__Inference__: Now that we have parameters estimated from the `training` data, we can use those parameters on the `test` dataset to see how well the model can differentiate between an actual banknote and a forgery on data it has never seen. We run the classification operation on the (unseen) test data [using the `svmpredict(...)` method](https://github.com/JuliaML/LIBSVM.jl/blob/master/src/LIBSVM.jl). \n",
    "* The [`svmpredict(...)` method](https://github.com/JuliaML/LIBSVM.jl/blob/master/src/LIBSVM.jl) returns the predicted label which we store in the `ŷ::Array{Int64,1}` array. We store the actual (correct) label in the `y::Array{Int64,1}` vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b42d7969-9bfa-4f4f-98fb-ea9443c11eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ,y,d = let\n",
    "\n",
    "     # Setup the data that we are using\n",
    "    D = test; # what dataset are we looking at?\n",
    "    number_of_examples = size(D,1); # how many rows?\n",
    "    X = [D[:,1:end-1] ones(number_of_examples)] |> transpose |> Matrix; # features (arranged as m x n)\n",
    "    y = D[:,end]; # label\n",
    "    \n",
    "    # Test model on the other half of the data.\n",
    "    ŷ, decision_values = svmpredict(model, X);\n",
    "\n",
    "    # return -\n",
    "    ŷ,y,decision_values\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efea4db2-103c-4e8f-b858-96dd1fb66db5",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "The confusion matrix is a $2\\times{2}$ matrix that contains four entries: true positive (TP), false positive (FP), true negative (TN), and false negative (FN). [Click me for a confusion matrix schematic!](https://github.com/varnerlab/CHEME-5820-Labs-Spring-2025/blob/main/labs/week-3/L3b/figs/Fig-BinaryConfusionMatrix.pdf). Let's compute these four values [using the `confusion(...)` method](src/Compute.jl) and store them in the `CM::Array{Int64,2}` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d57ea7ec-1fd8-41cf-9d4d-e85768e10d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Matrix{Int64}:\n",
       " 325   77\n",
       "  82  316"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CM = confusion(y, ŷ) # call with the SVM values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52750870-9c80-4e72-8dfa-6761ba710171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction correct: 0.80125 Fraction incorrect 0.19874999999999998\n"
     ]
    }
   ],
   "source": [
    "number_of_test_points = length(y);\n",
    "correct_prediction_perceptron = CM[1,1] + CM[2,2];\n",
    "(correct_prediction_perceptron/number_of_test_points) |> f-> println(\"Fraction correct: $(f) Fraction incorrect $(1-f)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0e2021-5686-473e-8f6b-a4508bb14a31",
   "metadata": {},
   "source": [
    "## Task 3: Implement Grid Search for Optimal Hyperparameters\n",
    "Fill me in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "916a6ac0-a18f-4b0e-9aa5-dda1e0fa7dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "error, α, β = let\n",
    "    \n",
    "    # Training data setup -\n",
    "    D₁ = training; # what dataset are we looking at?\n",
    "    number_of_training_examples = size(D₁,1); # how many rows?\n",
    "    X₁ = [D₁[:,1:end-1] ones(number_of_training_examples)] |> transpose |> Matrix; # augmented features (arranged as m x n)\n",
    "    y₁ = D₁[:,end]; # label\n",
    "\n",
    "    # Test data setup -\n",
    "    D₂ = test; # what dataset are we looking at?\n",
    "    number_of_test_examples = size(D₂,1); # how many rows?\n",
    "    X₂ = [D₂[:,1:end-1] ones(number_of_test_examples)] |> transpose |> Matrix; # features (arranged as m x n)\n",
    "    y₂ = D₂[:,end]; # label\n",
    "    \n",
    "    α = range(-5,stop = 15, step=2) |> collect; # exponent for C -\n",
    "    β = range(-15,stop = 3, step=2) |> collect; # exponent for γ -\n",
    "    number_of_points_C = length(α);\n",
    "    number_of_points_gamma = length(β);\n",
    "    error = Array{Float64,2}(undef, number_of_points_C, number_of_points_gamma);\n",
    "    \n",
    "    for i ∈ eachindex(α)\n",
    "        C = 2.0^α[i];\n",
    "        for j ∈ eachindex(β)\n",
    "            γ = 2.0^β[j];\n",
    "\n",
    "            # train model -\n",
    "            ŷ₂,_ = svmtrain(X₁, y₁, kernel=LIBSVM.Kernel.RadialBasis, \n",
    "                verbose = false, cost = C, gamma = γ) |> model -> svmpredict(model,X₂);\n",
    "\n",
    "            # how many mistakes?\n",
    "            CM = confusion(y₂, ŷ₂);\n",
    "            correct_prediction_perceptron = CM[1,1] + CM[2,2];\n",
    "            error[i,j] = (correct_prediction_perceptron/number_of_test_examples);\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    error, α, β\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bfccd256-16f9-4a9f-a649-f25f161fdb53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGQAAABuCAAAAAD0EunuAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAAURJREFUaAW9wbGNUwEABcH99vpAFEAnFEBn9EZIggSHRAHkJGRIn+hdYmRBsjN+4N/8YD4yB/OWec89CUhAAhKQgAQkIAEJyEPfmc/Mhbkyv3hEAhKQgAQkIAEJSEAC8hffmC/MyRzMwb1PzDtGAhKQgAQkIAEJSEAC8uIr88yczMFcmdfMG+ZgfjISkIAEJCABCUhAAhLwK/PMnDxyYWTeMAf3JCABCUhAAhKQgAQk4DNz8siNuTFPzMEjEpCABCQgAQlIQAIS8OSRK3NhXjFPzBPzm3sSkIAEJCABCUhAAhKQFwdzMjIHc2VuzJX5zT0JSEACEpCABCQgAQl4MCdzYS7MlbkyN/6NBCQgAQlIQAISkIAEPLl3Y07mysjcmJNHJCABCUhAAhKQgAQkIC8uzME9GflfEpCABCQgAQlIQAIS+ANesBsaLsXEdwAAAABJRU5ErkJggg==",
      "text/html": [
       "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAGQAAABuCAAAAAD0EunuAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAAURJREFUaAW9wbGNUwEABcH99vpAFEAnFEBn9EZIggSHRAHkJGRIn+hdYmRBsjN+4N/8YD4yB/OWec89CUhAAhKQgAQkIAEJyEPfmc/Mhbkyv3hEAhKQgAQkIAEJSEAC8hffmC/MyRzMwb1PzDtGAhKQgAQkIAEJSEAC8uIr88yczMFcmdfMG+ZgfjISkIAEJCABCUhAAhLwK/PMnDxyYWTeMAf3JCABCUhAAhKQgAQk4DNz8siNuTFPzMEjEpCABCQgAQlIQAIS8OSRK3NhXjFPzBPzm3sSkIAEJCABCUhAAhKQFwdzMjIHc2VuzJX5zT0JSEACEpCABCQgAQl4MCdzYS7MlbkyN/6NBCQgAQlIQAISkIAEPLl3Y07mysjcmJNHJCABCUhAAhKQgAQkIC8uzME9GflfEpCABCQgAQlIQAIS+ANesBsaLsXEdwAAAABJRU5ErkJg\">"
      ],
      "text/plain": [
       "11×10 Matrix{Gray{Float64}}:\n",
       " 0.5025   0.5025   0.5025   0.5025   …  0.1625   0.24625  0.5025   0.5025\n",
       " 0.5025   0.5025   0.5025   0.35        0.175    0.20375  0.5025   0.5025\n",
       " 0.5025   0.5025   0.33625  0.16125     0.18125  0.20375  0.295    0.5025\n",
       " 0.5025   0.33125  0.16125  0.15625     0.2125   0.2525   0.29625  0.43875\n",
       " 0.33125  0.16125  0.15875  0.1575      0.22625  0.28875  0.3      0.43875\n",
       " 0.16125  0.15875  0.15875  0.15875  …  0.2525   0.2925   0.3      0.43875\n",
       " 0.15875  0.15875  0.15875  0.1675      0.275    0.31875  0.3      0.43875\n",
       " 0.15875  0.16     0.1575   0.18375     0.295    0.33     0.3      0.43875\n",
       " 0.16     0.15875  0.16625  0.19125     0.315    0.33     0.3      0.43875\n",
       " 0.1575   0.15875  0.18375  0.1875      0.3325   0.33     0.3      0.43875\n",
       " 0.15875  0.16625  0.18875  0.19     …  0.335    0.33     0.3      0.43875"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Gray.(1 .- error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "476dbf34-3f9d-482e-a389-46b0325d6872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11×10 Matrix{Float64}:\n",
       " 0.4975   0.4975   0.4975   0.4975   …  0.8375   0.75375  0.4975   0.4975\n",
       " 0.4975   0.4975   0.4975   0.65        0.825    0.79625  0.4975   0.4975\n",
       " 0.4975   0.4975   0.66375  0.83875     0.81875  0.79625  0.705    0.4975\n",
       " 0.4975   0.66875  0.83875  0.84375     0.7875   0.7475   0.70375  0.56125\n",
       " 0.66875  0.83875  0.84125  0.8425      0.77375  0.71125  0.7      0.56125\n",
       " 0.83875  0.84125  0.84125  0.84125  …  0.7475   0.7075   0.7      0.56125\n",
       " 0.84125  0.84125  0.84125  0.8325      0.725    0.68125  0.7      0.56125\n",
       " 0.84125  0.84     0.8425   0.81625     0.705    0.67     0.7      0.56125\n",
       " 0.84     0.84125  0.83375  0.80875     0.685    0.67     0.7      0.56125\n",
       " 0.8425   0.84125  0.81625  0.8125      0.6675   0.67     0.7      0.56125\n",
       " 0.84125  0.83375  0.81125  0.81     …  0.665    0.67     0.7      0.56125"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b7cb67f-dcaa-4fcd-a800-389d8b580625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CartesianIndex(4, 4)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coordinate = argmax(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab4c8dd4-b872-413b-852c-4f0e578bdfa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = coordinate[1] |> i-> α[i] |> e-> 2.0^e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a16e8e86-49ee-41e9-b935-f8fce18f882e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001953125"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "γ = coordinate[2] |> i-> β[i] |> e-> 2.0^e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd48bd8d-b921-4ac1-a68b-0932373677c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.3",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
